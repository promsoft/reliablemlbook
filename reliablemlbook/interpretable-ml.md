# Объяснимое машинное обучение

Разрабатывая и внедряя ML-модели, мы фактически перепоручаем алгоритмам приняние решений. Нам нужно объяснять принятые решения другим участникам бизнес-процесса. Нам нужно контролировать качество принятых решения, а для этого хорошо бы понимать - как они были приняты. И - нам нужно разбираться с ошибками, работать над качеством моделей, данных и процессов.

eXplainable AI (XAI) - набор подходов и библиотек, позволяющих объяснять предсказания моделей машинного обучения и исследовать то, как они принимают решения. Иногда разделяют Explaination - т.е. объяснение процесса принятия решения, и Interpretation - аттрибутирование принятого решения входными признаками. Разницу можно понять на следующем примере:

Нейронная сеть - функция, вычислимая через последовательные матричные преобразования входных данных, и в этом смысле она полностью объяснима - мы можем проследить путь от входного признака до результата, но таких преобразований слишком много, они "не уместятся в голове" пользователей. С другой стороны, интерпретация может звучать как "эта нейронная сеть хорошо определяет пол взрослых животных, а на детенышах она путается" - что будет, скорее всего, очень вольным описанием происходящего - зато понятным для пользователя.

Инструменты XAI постоянно развиваются. За подробным описанием мы отсылаем читателя к книге Кристофа Мольнара "Interpretable Machine Learning". Здесь мы хотели бы остановиться на некоторых основополагающих подходах, которые полезно понимать и использовать.
